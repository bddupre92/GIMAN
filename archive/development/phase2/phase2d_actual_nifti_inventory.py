#!/usr/bin/env python3
"""Phase 2D: ACTUAL NIfTI File Inventory

CRITICAL: Count real NIfTI files available on disk, not CSV metadata!
This script will scan all directories and find every available T1-weighted image.

Author: GIMAN Development Team
Date: September 27, 2025
Phase: 2D - Real File Inventory
"""

import logging
import os
import re
import warnings
from collections import defaultdict
from pathlib import Path

import pandas as pd

warnings.filterwarnings("ignore")

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


class ActualNIfTIInventory:
    """Inventory actual NIfTI files available on disk."""

    def __init__(self, base_dir: str):
        """Initialize NIfTI inventory."""
        self.base_dir = Path(base_dir)
        logger.info("üîç ACTUAL NIfTI FILE INVENTORY started")
        logger.info(f"üìÅ Base directory: {self.base_dir}")

        # T1-weighted sequence patterns to look for
        self.t1_patterns = [
            r".*mprage.*\.nii(\.gz)?$",  # MPRAGE
            r".*t1.*\.nii(\.gz)?$",  # Generic T1
            r".*spgr.*\.nii(\.gz)?$",  # SPGR
            r".*flair.*\.nii(\.gz)?$",  # T1-FLAIR
            r".*3d.*t1.*\.nii(\.gz)?$",  # 3D T1
            r".*structural.*\.nii(\.gz)?$",  # Structural
            r".*anat.*\.nii(\.gz)?$",  # Anatomical
        ]

        # DaTSCAN patterns
        self.datscan_patterns = [
            r".*datscan.*\.nii(\.gz)?$",
            r".*spect.*\.nii(\.gz)?$",
            r".*dat.*\.nii(\.gz)?$",
        ]

    def scan_directory_tree(self) -> dict:
        """Recursively scan directory tree for NIfTI files."""
        logger.info("üîç Scanning directory tree for NIfTI files...")

        file_inventory = {
            "T1_files": [],
            "DaTSCAN_files": [],
            "other_nifti": [],
            "directories_scanned": 0,
            "total_files_found": 0,
        }

        try:
            # Walk through all directories
            for root, dirs, files in os.walk(self.base_dir):
                file_inventory["directories_scanned"] += 1

                # Log progress every 100 directories
                if file_inventory["directories_scanned"] % 100 == 0:
                    logger.info(
                        f"üìÅ Scanned {file_inventory['directories_scanned']} directories..."
                    )

                for file in files:
                    if file.endswith((".nii", ".nii.gz")):
                        file_inventory["total_files_found"] += 1
                        full_path = Path(root) / file
                        file_lower = file.lower()

                        # Check if it's T1-weighted
                        is_t1 = any(
                            re.match(pattern, file_lower, re.IGNORECASE)
                            for pattern in self.t1_patterns
                        )

                        # Check if it's DaTSCAN
                        is_datscan = any(
                            re.match(pattern, file_lower, re.IGNORECASE)
                            for pattern in self.datscan_patterns
                        )

                        if is_t1:
                            file_inventory["T1_files"].append(
                                {
                                    "path": str(full_path),
                                    "filename": file,
                                    "directory": str(Path(root)),
                                    "sequence_type": self._identify_t1_type(file_lower),
                                }
                            )
                        elif is_datscan:
                            file_inventory["DaTSCAN_files"].append(
                                {
                                    "path": str(full_path),
                                    "filename": file,
                                    "directory": str(Path(root)),
                                }
                            )
                        else:
                            file_inventory["other_nifti"].append(
                                {
                                    "path": str(full_path),
                                    "filename": file,
                                    "directory": str(Path(root)),
                                }
                            )

        except Exception as e:
            logger.error(f"‚ùå Error scanning directories: {e}")

        logger.info("‚úÖ Directory scan complete!")
        logger.info(f"üìÅ Directories scanned: {file_inventory['directories_scanned']}")
        logger.info(
            f"üìÑ Total NIfTI files found: {file_inventory['total_files_found']}"
        )
        logger.info(f"üß† T1-weighted files: {len(file_inventory['T1_files'])}")
        logger.info(f"üíâ DaTSCAN files: {len(file_inventory['DaTSCAN_files'])}")
        logger.info(f"üî¨ Other NIfTI files: {len(file_inventory['other_nifti'])}")

        return file_inventory

    def _identify_t1_type(self, filename: str) -> str:
        """Identify specific T1 sequence type from filename."""
        filename = filename.lower()

        if "mprage" in filename:
            return "MPRAGE"
        elif "spgr" in filename:
            return "SPGR"
        elif "flair" in filename:
            return "T1-FLAIR"
        elif "3d" in filename and "t1" in filename:
            return "3D-T1"
        elif "structural" in filename:
            return "Structural"
        elif "anat" in filename:
            return "Anatomical"
        elif "t1" in filename:
            return "T1-Generic"
        else:
            return "T1-Unknown"

    def extract_patient_ids(self, file_inventory: dict) -> dict:
        """Extract patient IDs from file paths to match with clinical data."""
        logger.info("üÜî Extracting patient IDs from file paths...")

        # Common PPMI patient ID patterns
        patno_patterns = [
            r"(\d{4,5})",  # 4-5 digit numbers (typical PPMI patient IDs)
            r"PATNO_(\d+)",
            r"PATIENT_(\d+)",
            r"ID_(\d+)",
        ]

        patient_mapping = defaultdict(lambda: {"T1_files": [], "DaTSCAN_files": []})

        # Process T1 files
        for t1_file in file_inventory["T1_files"]:
            path = t1_file["path"]

            # Try to extract patient ID from path
            for pattern in patno_patterns:
                matches = re.findall(pattern, path)
                if matches:
                    # Take the first reasonable match (4-5 digits)
                    for match in matches:
                        if len(match) >= 4 and len(match) <= 5:
                            patno = int(match)
                            patient_mapping[patno]["T1_files"].append(t1_file)
                            break
                    break

        # Process DaTSCAN files
        for datscan_file in file_inventory["DaTSCAN_files"]:
            path = datscan_file["path"]

            # Try to extract patient ID from path
            for pattern in patno_patterns:
                matches = re.findall(pattern, path)
                if matches:
                    # Take the first reasonable match (4-5 digits)
                    for match in matches:
                        if len(match) >= 4 and len(match) <= 5:
                            patno = int(match)
                            patient_mapping[patno]["DaTSCAN_files"].append(datscan_file)
                            break
                    break

        logger.info(f"üÜî Patient mapping created for {len(patient_mapping)} patients")

        # Count patients with both T1 and DaTSCAN
        both_modalities = 0
        t1_only = 0
        datscan_only = 0

        for patno, files in patient_mapping.items():
            has_t1 = len(files["T1_files"]) > 0
            has_datscan = len(files["DaTSCAN_files"]) > 0

            if has_t1 and has_datscan:
                both_modalities += 1
            elif has_t1:
                t1_only += 1
            elif has_datscan:
                datscan_only += 1

        logger.info("üìä Patient breakdown:")
        logger.info(f"   Both T1 + DaTSCAN: {both_modalities} patients")
        logger.info(f"   T1 only: {t1_only} patients")
        logger.info(f"   DaTSCAN only: {datscan_only} patients")

        return dict(patient_mapping)

    def analyze_t1_sequence_diversity(self, patient_mapping: dict) -> dict:
        """Analyze diversity of T1 sequence types available."""
        logger.info("üìä Analyzing T1 sequence diversity...")

        sequence_counts = defaultdict(int)
        sequence_patients = defaultdict(set)

        for patno, files in patient_mapping.items():
            for t1_file in files["T1_files"]:
                seq_type = t1_file["sequence_type"]
                sequence_counts[seq_type] += 1
                sequence_patients[seq_type].add(patno)

        diversity_analysis = {
            "sequence_types": dict(sequence_counts),
            "patients_per_sequence": {
                seq: len(patients) for seq, patients in sequence_patients.items()
            },
            "total_t1_files": sum(sequence_counts.values()),
            "unique_sequences": len(sequence_counts),
        }

        logger.info("üß† T1 Sequence Analysis:")
        for seq_type, count in sequence_counts.items():
            patient_count = len(sequence_patients[seq_type])
            logger.info(f"   {seq_type}: {count} files from {patient_count} patients")

        return diversity_analysis

    def create_expansion_cohort(self, patient_mapping: dict) -> pd.DataFrame:
        """Create actual expansion cohort based on available files."""
        logger.info("üéØ Creating expansion cohort from available files...")

        cohort_data = []

        for patno, files in patient_mapping.items():
            has_t1 = len(files["T1_files"]) > 0
            has_datscan = len(files["DaTSCAN_files"]) > 0

            # For now, include all patients with T1 (DaTSCAN optional for T1 expansion)
            if has_t1:
                t1_sequences = [f["sequence_type"] for f in files["T1_files"]]
                t1_files = [f["path"] for f in files["T1_files"]]
                datscan_files = [f["path"] for f in files["DaTSCAN_files"]]

                cohort_data.append(
                    {
                        "PATNO": patno,
                        "has_T1": has_t1,
                        "has_DaTSCAN": has_datscan,
                        "T1_count": len(files["T1_files"]),
                        "DaTSCAN_count": len(files["DaTSCAN_files"]),
                        "T1_sequences": ",".join(set(t1_sequences)),
                        "T1_files": "|".join(t1_files),
                        "DaTSCAN_files": "|".join(datscan_files)
                        if datscan_files
                        else "",
                        "multimodal": has_t1 and has_datscan,
                    }
                )

        cohort_df = pd.DataFrame(cohort_data)

        logger.info("üéØ Expansion cohort created:")
        logger.info(f"   Total patients with T1: {len(cohort_df)}")
        logger.info(
            f"   Patients with both T1+DaTSCAN: {cohort_df['multimodal'].sum()}"
        )
        logger.info(f"   T1-only patients: {(~cohort_df['multimodal']).sum()}")

        return cohort_df

    def generate_realistic_expansion_plan(
        self, cohort_df: pd.DataFrame, diversity_analysis: dict
    ) -> str:
        """Generate realistic expansion plan based on actual file availability."""
        n_total_t1 = len(cohort_df)
        n_multimodal = cohort_df["multimodal"].sum()
        n_t1_only = n_total_t1 - n_multimodal

        current_dataset = 95  # Current GIMAN dataset size

        expansion_plan = f"""
# üéØ REALISTIC T1 EXPANSION PLAN (Based on Actual Files)

## üìä ACTUAL FILE INVENTORY RESULTS
- **Total T1 Files Available**: {diversity_analysis["total_t1_files"]} files
- **Unique T1 Patients**: {n_total_t1} patients
- **Current GIMAN Dataset**: {current_dataset} patients
- **Realistic Expansion**: {current_dataset} ‚Üí {n_total_t1} ({n_total_t1 / current_dataset:.1f}x)

## üß† T1 SEQUENCE DIVERSITY (Available)
"""

        for seq_type, count in diversity_analysis["sequence_types"].items():
            patient_count = diversity_analysis["patients_per_sequence"][seq_type]
            expansion_plan += (
                f"- **{seq_type}**: {count} files from {patient_count} patients\n"
            )

        expansion_plan += f"""

## üéØ EXPANSION STRATEGY OPTIONS

### Option 1: T1-Only Expansion (Maximum Dataset)
- **Patients**: {n_total_t1} (all T1 patients)
- **Expansion Factor**: {n_total_t1 / current_dataset:.1f}x
- **Advantage**: Maximum statistical power
- **Challenge**: Some patients lack DaTSCAN (T1 features only)

### Option 2: Multimodal Expansion (Conservative)
- **Patients**: {n_multimodal} (T1 + DaTSCAN only)
- **Expansion Factor**: {n_multimodal / current_dataset:.1f}x
- **Advantage**: Maintains multimodal architecture
- **Challenge**: Smaller expansion than Option 1

### Option 3: Hybrid Approach (Recommended)
- **Phase 1**: Test T1-only model on {n_total_t1} patients
- **Phase 2**: Test multimodal model on {n_multimodal} patients  
- **Phase 3**: Ensemble approach combining both
- **Advantage**: Best of both worlds

## üìã IMMEDIATE IMPLEMENTATION (Next 24-48 Hours)

### Step 1: Validate File Quality (4 hours)
```python
# Check NIfTI file integrity
# Verify image dimensions and orientations
# Identify any corrupted files
```

### Step 2: Implement T1 Harmonization (6 hours)
```python
# Intensity normalization across sequence types
# Spatial registration to common template
# Quality control pipeline
```

### Step 3: Test Simplified Ensemble (2 hours)
```python
# Run Phase 5 simplified ensemble on expanded T1 dataset
# Expected: R¬≤ improvement from -0.0189 to positive
```

### Step 4: Architecture Validation (4 hours)
```python
# Test GAT architectures on expanded dataset
# Compare T1-only vs multimodal performance
```

## üéâ REALISTIC EXPECTATIONS

### Conservative Estimate (Option 2: {n_multimodal} patients):
- **R¬≤ Improvement**: -0.0189 ‚Üí +0.10-0.20
- **AUC Improvement**: 0.56 ‚Üí 0.65-0.70
- **Statistical Significance**: p < 0.05

### Optimistic Estimate (Option 1: {n_total_t1} patients):
- **R¬≤ Improvement**: -0.0189 ‚Üí +0.15-0.25
- **AUC Improvement**: 0.56 ‚Üí 0.70-0.75
- **Statistical Significance**: p < 0.001

## üöÄ SUCCESS PROBABILITY
Based on actual file availability:
- **High Confidence**: {n_multimodal} patient expansion will improve performance
- **Medium Confidence**: {n_total_t1} patient expansion will achieve target R¬≤ > 0.15
- **Realistic Timeline**: 48-72 hours for initial validation
"""

        return expansion_plan


def run_actual_nifti_inventory():
    """Execute actual NIfTI file inventory and create realistic expansion plan."""
    logger.info("üîç ACTUAL NIfTI FILE INVENTORY EXECUTION")
    logger.info("=" * 60)

    # Define search directories (adjust based on your file structure)
    possible_dirs = [
        "/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/data",
        "/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/archive/data",
        "/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025",
    ]

    # Find the first existing directory
    base_dir = None
    for dir_path in possible_dirs:
        if Path(dir_path).exists():
            base_dir = dir_path
            break

    if base_dir is None:
        logger.error("‚ùå No valid data directory found!")
        logger.info("üìÅ Please update the directory paths in the script")
        return None, None, None

    # Initialize inventory
    inventory = ActualNIfTIInventory(base_dir)

    # Scan for files
    file_inventory = inventory.scan_directory_tree()

    if file_inventory["total_files_found"] == 0:
        logger.warning("‚ö†Ô∏è No NIfTI files found! Check directory paths")
        return file_inventory, None, None

    # Extract patient mapping
    patient_mapping = inventory.extract_patient_ids(file_inventory)

    # Analyze T1 diversity
    diversity_analysis = inventory.analyze_t1_sequence_diversity(patient_mapping)

    # Create expansion cohort
    cohort_df = inventory.create_expansion_cohort(patient_mapping)

    # Generate realistic plan
    expansion_plan = inventory.generate_realistic_expansion_plan(
        cohort_df, diversity_analysis
    )

    logger.info("\nüìã REALISTIC T1 EXPANSION PLAN:")
    print(expansion_plan)

    # Save results
    output_dir = Path(
        "/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/archive/development/phase2"
    )

    # Save cohort data
    cohort_path = output_dir / "actual_t1_expansion_cohort.csv"
    cohort_df.to_csv(cohort_path, index=False)
    logger.info(f"üíæ Actual expansion cohort saved: {cohort_path}")

    # Save file inventory
    import json

    inventory_path = output_dir / "nifti_file_inventory.json"
    with open(inventory_path, "w") as f:
        # Convert Path objects to strings for JSON serialization
        json_inventory = {
            "T1_files": file_inventory["T1_files"],
            "DaTSCAN_files": file_inventory["DaTSCAN_files"],
            "other_nifti": file_inventory["other_nifti"][:100],  # Limit size
            "summary": {
                "directories_scanned": file_inventory["directories_scanned"],
                "total_files_found": file_inventory["total_files_found"],
                "T1_count": len(file_inventory["T1_files"]),
                "DaTSCAN_count": len(file_inventory["DaTSCAN_files"]),
            },
        }
        json.dump(json_inventory, f, indent=2)
    logger.info(f"üíæ File inventory saved: {inventory_path}")

    return file_inventory, patient_mapping, cohort_df


if __name__ == "__main__":
    file_inventory, patient_mapping, cohort_df = run_actual_nifti_inventory()

    if cohort_df is not None:
        print("\nüéâ ACTUAL T1 EXPANSION READY!")
        print(f"Real files found: {len(cohort_df)} T1 patients")
        print(f"Multimodal patients: {cohort_df['multimodal'].sum()}")
        print(f"Realistic expansion: 95 ‚Üí {len(cohort_df)} patients")
        print("Ready for immediate file validation and testing!")
