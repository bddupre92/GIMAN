"""Real PPMI Data Integration Test for GIMAN Phase 1.

This script tests integration with the actual preprocessed PPMI dataset:
- 557 patients with 7 biomarker features
- Patient similarity graph with ~44K edges
- PD vs Healthy Control classification
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import networkx as nx
import numpy as np
import pandas as pd
import torch
from sklearn.impute import SimpleImputer

from src.giman_pipeline.training.data_loaders import create_pyg_data
from src.giman_pipeline.training.models import create_giman_model


def load_real_ppmi_data():
    """Load the actual preprocessed PPMI dataset."""
    print("üîÑ Loading real PPMI data...")

    data_dir = Path("data/02_processed")

    # Find the biomarker imputation file (with timestamp)
    pattern = "giman_biomarker_imputed_*_patients_*.csv"
    files = list(data_dir.glob(pattern))

    if not files:
        raise FileNotFoundError(f"No imputed biomarker files found matching {pattern}")

    # Use the most recent file
    biomarker_file = sorted(files)[-1]
    print(f"   - Loading: {biomarker_file.name}")

    # Load biomarker data
    df = pd.read_csv(biomarker_file)
    print(f"   - Raw dataset shape: {df.shape}")

    # Remove duplicates (keep first occurrence)
    df = df.drop_duplicates(subset=["PATNO"], keep="first")
    print(f"   - After deduplication: {df.shape}")

    # Check for missing values in biomarker columns
    biomarker_features = [
        "LRRK2",
        "GBA",
        "APOE_RISK",
        "PTAU",
        "TTAU",
        "UPSIT_TOTAL",
        "ALPHA_SYN",
    ]

    print("   - Checking biomarker completeness:")
    for feature in biomarker_features:
        missing_count = df[feature].isna().sum()
        print(f"     * {feature}: {missing_count} missing values")

    # Fill any remaining NaN values with column median
    for feature in biomarker_features:
        if df[feature].isna().any():
            median_val = df[feature].median()
            df[feature].fillna(median_val, inplace=True)
            print(f"     * Filled {feature} NaNs with median: {median_val:.3f}")

    pd_count = (df['COHORT_DEFINITION'] == "Parkinson's Disease").sum()
    hc_count = (df['COHORT_DEFINITION'] == 'Healthy Control').sum()
    print(f"   - PD cases: {pd_count}")
    print(f"   - Healthy controls: {hc_count}")

    return df


def create_similarity_graph(patient_data, similarity_threshold=0.3):
    """Create patient similarity graph from biomarker data."""
    print(f"üîÑ Creating similarity graph (threshold={similarity_threshold})...")

    # Get biomarker features
    biomarker_features = [
        "LRRK2",
        "GBA",
        "APOE_RISK",
        "PTAU",
        "TTAU",
        "UPSIT_TOTAL",
        "ALPHA_SYN",
    ]
    feature_matrix = patient_data[biomarker_features].values

    # Check for NaN values
    if np.isnan(feature_matrix).any():
        print("   - Warning: Found NaN values in feature matrix")
        nan_counts = np.isnan(feature_matrix).sum(axis=0)
        for i, feature in enumerate(biomarker_features):
            if nan_counts[i] > 0:
                print(f"     * {feature}: {nan_counts[i]} NaN values")

        # Replace NaN with column medians
        imputer = SimpleImputer(strategy="median")
        feature_matrix = imputer.fit_transform(feature_matrix)
        print("   - Imputed NaN values with column medians")

    # Compute pairwise cosine similarities
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.preprocessing import StandardScaler

    # Standardize features
    scaler = StandardScaler()
    feature_matrix_scaled = scaler.fit_transform(feature_matrix)

    # Compute similarity matrix
    similarity_matrix = cosine_similarity(feature_matrix_scaled)

    # Create NetworkX graph
    G = nx.Graph()

    # Add nodes (patients)
    for i, patient_id in enumerate(patient_data["PATNO"]):
        G.add_node(i, patient_id=patient_id)

    # Add edges above threshold
    edges_added = 0
    for i in range(len(patient_data)):
        for j in range(i + 1, len(patient_data)):
            similarity = similarity_matrix[i, j]
            if similarity >= similarity_threshold:
                G.add_edge(i, j, weight=similarity)
                edges_added += 1

    print(f"   - Graph nodes: {G.number_of_nodes()}")
    print(f"   - Graph edges: {G.number_of_edges()}")
    print(f"   - Graph density: {nx.density(G):.4f}")

    return G


def test_real_data_integration():
    """Test GIMAN with real PPMI data."""
    print("=" * 60)
    print("üß¨ GIMAN REAL PPMI DATA INTEGRATION TEST")
    print("=" * 60)

    # Test 1: Load Real Data
    print("\n1Ô∏è‚É£ Loading Real PPMI Data...")
    try:
        patient_data = load_real_ppmi_data()
        print("‚úÖ Real data loaded successfully!")

    except Exception as e:
        print(f"‚ùå Real data loading failed: {e}")
        return False

    # Test 2: Create Similarity Graph
    print("\n2Ô∏è‚É£ Creating Patient Similarity Graph...")
    try:
        _ = create_similarity_graph(
            patient_data, similarity_threshold=0.3
        )
        print("‚úÖ Similarity graph created successfully!")

    except Exception as e:
        print(f"‚ùå Similarity graph creation failed: {e}")
        return False

    # Initialize variables for later use
    pyg_data = None
    model = None
    filtered_graph = None

    # Test 3: Convert to PyG Format
    print("\n3Ô∏è‚É£ Converting to PyTorch Geometric...")
    try:
        # Check cohort labels first
        cohort_labels = patient_data["COHORT_DEFINITION"].unique()
        print(f"   - Found cohort labels: {cohort_labels}")

        # Filter to only PD and Healthy Control for now
        filtered_data = patient_data[
            patient_data["COHORT_DEFINITION"].isin(
                ["Parkinson's Disease", "Healthy Control"]
            )
        ].copy()

        print(f"   - Filtered to PD/HC: {len(filtered_data)} patients")

        # Recreate similarity graph with filtered data
        print("   - Recreating similarity graph for filtered data...")
        filtered_graph = create_similarity_graph(
            filtered_data, similarity_threshold=0.3
        )

        pyg_data = create_pyg_data(
            similarity_graph=filtered_graph,
            patient_data=filtered_data,
            biomarker_features=[
                "LRRK2",
                "GBA",
                "APOE_RISK",
                "PTAU",
                "TTAU",
                "UPSIT_TOTAL",
                "ALPHA_SYN",
            ],
            standardize_features=True,
        )

        print("‚úÖ PyG conversion successful!")
        print(f"   - Nodes: {pyg_data.num_nodes}")
        print(f"   - Edges: {pyg_data.num_edges}")
        print(f"   - Node features: {pyg_data.x.shape}")
        print(f"   - Edge features: {pyg_data.edge_attr.shape}")
        print(f"   - Labels: {pyg_data.y.shape}")
        print(f"   - PD cases: {(pyg_data.y == 1).sum().item()}")
        print(f"   - Healthy controls: {(pyg_data.y == 0).sum().item()}")

    except Exception as e:
        print(f"‚ùå PyG conversion failed: {e}")
        return False

    # Test 4: Initialize GIMAN Model
    print("\n4Ô∏è‚É£ Creating GIMAN Model...")
    try:
        model = create_giman_model(
            input_dim=7,
            hidden_dims=[64, 128, 64],
            output_dim=2,
            dropout_rate=0.3,
            pooling_method="concat",
        )

        print("‚úÖ GIMAN model created successfully!")

    except Exception as e:
        print(f"‚ùå Model creation failed: {e}")
        return False

    # Test 5: Forward Pass with Real Data
    print("\n5Ô∏è‚É£ Testing Forward Pass with Real Data...")
    try:
        model.eval()

        with torch.no_grad():
            output = model.forward(pyg_data)

        print("‚úÖ Forward pass with real data successful!")
        print(f"   - Logits shape: {output['logits'].shape}")
        print(f"   - Node embeddings: {output['node_embeddings'].shape}")
        print(f"   - Graph embedding: {output['graph_embedding'].shape}")

        # Validate outputs
        assert output["logits"].shape == (1, 2), "Wrong logits shape"
        assert output["node_embeddings"].shape == (pyg_data.num_nodes, 64), (
            "Wrong node embeddings shape"
        )
        assert len(output["layer_embeddings"]) == 3, "Wrong number of layer embeddings"

    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        return False

    # Test 6: Predictions on Real Data
    print("\n6Ô∏è‚É£ Testing Predictions on Real Data...")
    try:
        probabilities = model.predict_proba(pyg_data)
        predictions = model.predict(pyg_data)

        print("‚úÖ Predictions on real data successful!")
        print(f"   - Prediction probabilities: {probabilities.squeeze()}")
        print(
            f"   - Predicted class: {'PD' if predictions.item() == 1 else 'Healthy Control'}"
        )
        print(f"   - Confidence: {probabilities.max().item():.3f}")

    except Exception as e:
        print(f"‚ùå Predictions failed: {e}")
        return False

    # Test 7: Analyze Graph Statistics
    print("\n7Ô∏è‚É£ Analyzing Graph Structure...")
    try:
        # Use the filtered graph
        print(f"   - Graph density: {nx.density(filtered_graph):.4f}")
        print(
            f"   - Average clustering coefficient: {nx.average_clustering(filtered_graph):.4f}"
        )
        print(
            f"   - Number of connected components: {nx.number_connected_components(filtered_graph)}"
        )

        # Degree statistics
        degrees = dict(filtered_graph.degree())
        avg_degree = np.mean(list(degrees.values()))
        print(f"   - Average degree: {avg_degree:.2f}")
        print(f"   - Max degree: {max(degrees.values())}")
        print(f"   - Min degree: {min(degrees.values())}")

        # Edge weight statistics
        edge_weights = [
            data["weight"] for _, _, data in filtered_graph.edges(data=True)
        ]
        print(
            f"   - Edge weight range: [{min(edge_weights):.3f}, {max(edge_weights):.3f}]"
        )
        print(f"   - Average edge weight: {np.mean(edge_weights):.3f}")

    except Exception as e:
        print(f"‚ö†Ô∏è  Graph analysis failed (not critical): {e}")

    # Test 8: Memory and Performance
    print("\n8Ô∏è‚É£ Performance Analysis...")
    try:
        import time

        model.eval()
        start_time = time.time()

        for _ in range(10):
            with torch.no_grad():
                _ = model.forward(pyg_data)

        avg_time = (time.time() - start_time) / 10
        print("‚úÖ Performance test completed!")
        print(f"   - Average inference time: {avg_time * 1000:.2f} ms")
        print(
            f"   - Memory usage: ~{torch.cuda.memory_allocated() if torch.cuda.is_available() else 'N/A (CPU)'}"
        )

    except Exception as e:
        print(f"‚ö†Ô∏è  Performance test failed (not critical): {e}")

    # Final Summary
    print("\n" + "=" * 60)
    print("üéâ REAL PPMI DATA INTEGRATION SUCCESS!")
    print("=" * 60)
    print(f"‚úÖ Successfully processed {pyg_data.num_nodes} PPMI patients")
    print(f"‚úÖ Created similarity graph with {pyg_data.num_edges:,} edges")
    print(
        f"‚úÖ GIMAN model with {model.get_model_info()['total_parameters']:,} parameters"
    )
    print("‚úÖ Forward pass and predictions working on real data")
    print("‚úÖ Graph structure analysis completed")

    return True


if __name__ == "__main__":
    print("üöÄ Starting GIMAN Real Data Integration Test...")

    success = test_real_data_integration()

    if success:
        print("\nüéØ PHASE 1 COMPLETE - READY FOR TRAINING!")
        print("\nPhase 1 Achievements:")
        print("- ‚úÖ GNN backbone architecture implemented")
        print("- ‚úÖ Real PPMI data integration working")
        print("- ‚úÖ 557 patients √ó 7 biomarkers processing")
        print("- ‚úÖ Patient similarity graph with 44K+ edges")
        print("- ‚úÖ End-to-end inference pipeline")

        print("\nüöÄ Next: Phase 2 - Training Pipeline")
        print("- Implement loss functions and optimizers")
        print("- Add training/validation loops")
        print("- Create evaluation metrics")
        print("- Add experiment tracking")
    else:
        print("\n‚ùå Real data integration failed. Please fix errors.")
        sys.exit(1)
