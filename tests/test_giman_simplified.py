"""Simplified GIMAN Phase 1 test using existing preprocessed data.

This test uses the actual preprocessed files with timestamp suffixes
and validates the core GNN backbone functionality.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))


import pandas as pd
import torch


def test_simplified_giman():
    """Test GIMAN components without full data pipeline."""
    print("=" * 60)
    print("üß™ SIMPLIFIED GIMAN PHASE 1 TEST")
    print("=" * 60)

    # Test 1: Import Core Components
    print("\n1Ô∏è‚É£ Testing Core Imports...")
    try:
        import numpy as np
        import torch
        from torch_geometric.data import Data

        from src.giman_pipeline.training.models import (
            create_giman_model,
        )

        print("‚úÖ All imports successful!")

    except Exception as e:
        print(f"‚ùå Import failed: {e}")
        return False

    # Test 2: Create Model
    print("\n2Ô∏è‚É£ Testing Model Creation...")
    try:
        model = create_giman_model(
            input_dim=7,
            hidden_dims=[64, 128, 64],
            output_dim=2,
            dropout_rate=0.3,
            pooling_method="concat",
        )

        print("‚úÖ Model creation successful!")
        print(f"   - Total parameters: {model.get_model_info()['total_parameters']:,}")

    except Exception as e:
        print(f"‚ùå Model creation failed: {e}")
        return False

    # Test 3: Create Synthetic Data for Testing
    print("\n3Ô∏è‚É£ Creating Synthetic Test Data...")
    try:
        # Create synthetic patient data
        num_patients = 100
        num_features = 7

        # Synthetic biomarker features
        np.random.seed(42)
        x = torch.FloatTensor(np.random.randn(num_patients, num_features))

        # Create synthetic graph edges (random connectivity)
        num_edges = 500
        edge_list = []
        for _ in range(num_edges):
            u = np.random.randint(0, num_patients)
            v = np.random.randint(0, num_patients)
            if u != v:  # No self-loops
                edge_list.append([u, v])

        edge_index = torch.LongTensor(edge_list).t().contiguous()
        edge_attr = torch.FloatTensor(np.random.uniform(0.3, 0.8, len(edge_list)))

        # Synthetic labels (PD vs Control)
        y = torch.LongTensor(np.random.binomial(1, 0.4, num_patients))  # 40% PD cases

        # Create PyG Data object
        test_data = Data(
            x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, num_nodes=num_patients
        )

        print("‚úÖ Synthetic data created!")
        print(f"   - Patients: {test_data.num_nodes}")
        print(f"   - Edges: {test_data.num_edges}")
        print(f"   - Features: {test_data.x.shape}")
        print(f"   - PD cases: {(test_data.y == 1).sum()}")

    except Exception as e:
        print(f"‚ùå Synthetic data creation failed: {e}")
        return False

    # Test 4: Forward Pass
    print("\n4Ô∏è‚É£ Testing Forward Pass...")
    try:
        model.eval()

        with torch.no_grad():
            output = model.forward(test_data)

        print("‚úÖ Forward pass successful!")
        print(f"   - Logits shape: {output['logits'].shape}")
        print(f"   - Node embeddings: {output['node_embeddings'].shape}")
        print(f"   - Graph embedding: {output['graph_embedding'].shape}")

        # Validate shapes
        assert output["logits"].shape == (
            1,
            2,
        ), f"Wrong logits shape: {output['logits'].shape}"
        assert output["node_embeddings"].shape == (
            num_patients,
            64,
        ), "Wrong node embeddings shape"
        assert len(output["layer_embeddings"]) == 3, "Should have 3 layer embeddings"

    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        return False

    # Test 5: Predictions
    print("\n5Ô∏è‚É£ Testing Predictions...")
    try:
        probabilities = model.predict_proba(test_data)
        predictions = model.predict(test_data)

        print("‚úÖ Predictions successful!")
        print(f"   - Probabilities: {probabilities.squeeze()}")
        print(f"   - Predicted class: {predictions.item()}")

        # Validate
        assert probabilities.shape == (1, 2), "Wrong probability shape"
        assert torch.allclose(probabilities.sum(dim=1), torch.ones(1)), (
            "Probabilities don't sum to 1"
        )

    except Exception as e:
        print(f"‚ùå Predictions failed: {e}")
        return False

    # Test 6: Multiple Graph Sizes
    print("\n6Ô∏è‚É£ Testing Different Graph Sizes...")
    try:
        # Test with smaller graph
        small_data = Data(
            x=torch.randn(10, 7),
            edge_index=torch.LongTensor([[0, 1, 2], [1, 2, 0]]),
            edge_attr=torch.FloatTensor([0.5, 0.6, 0.7]),
            y=torch.LongTensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),
            num_nodes=10,
        )

        with torch.no_grad():
            small_output = model.forward(small_data)

        print("‚úÖ Small graph test passed!")
        print(f"   - Small graph logits: {small_output['logits'].shape}")

    except Exception as e:
        print(f"‚ùå Small graph test failed: {e}")
        return False

    # Test 7: Load Real Data (if available)
    print("\n7Ô∏è‚É£ Testing Real Data Loading (if available)...")
    try:
        data_dir = Path("data/02_processed")
        pattern = "giman_biomarker_imputed_*_patients_*.csv"
        files = list(data_dir.glob(pattern))

        if files:
            real_data_file = files[0]  # Use the most recent file
            print(f"   - Found real data: {real_data_file.name}")

            # Load real data
            df = pd.read_csv(real_data_file)
            print(f"   - Real data shape: {df.shape}")

            # Check required columns
            required_cols = [
                "PATNO",
                "COHORT_DEFINITION",
                "LRRK2",
                "GBA",
                "APOE_RISK",
                "PTAU",
                "TTAU",
                "UPSIT_TOTAL",
                "ALPHA_SYN",
            ]
            missing_cols = [col for col in required_cols if col not in df.columns]

            if not missing_cols:
                print("‚úÖ Real data has all required columns!")
                pd_count = (df["COHORT_DEFINITION"] == "Parkinson's Disease").sum()
                print(f"   - PD cases: {pd_count}")
                print(
                    f"   - Healthy controls: {(df['COHORT_DEFINITION'] == 'Healthy Control').sum()}"
                )
            else:
                print(f"‚ö†Ô∏è  Missing columns in real data: {missing_cols}")
        else:
            print("‚ÑπÔ∏è  No real preprocessed data found - using synthetic data only")

    except Exception as e:
        print(f"‚ö†Ô∏è  Real data loading failed (not critical): {e}")

    # Final Summary
    print("\n" + "=" * 60)
    print("üéâ PHASE 1 CORE FUNCTIONALITY VALIDATED!")
    print("=" * 60)
    print("‚úÖ GIMAN imports work correctly")
    print("‚úÖ Model creation successful")
    print("‚úÖ Forward pass produces valid outputs")
    print("‚úÖ Predictions work correctly")
    print("‚úÖ Architecture handles different graph sizes")
    print("\nüöÄ Ready to integrate with real PPMI data!")

    return True


def test_model_components():
    """Test individual model components."""
    print("\nüîß Testing Model Components...")

    try:
        from src.giman_pipeline.training.models import GIMANBackbone

        backbone = GIMANBackbone(
            input_dim=7,
            hidden_dims=[64, 128, 64],
            output_dim=2,
            dropout_rate=0.3,
            pooling_method="concat",
            use_residual=True,
        )

        # Test with synthetic data
        x = torch.randn(50, 7)
        edge_index = torch.LongTensor([[0, 1, 2, 1], [1, 2, 0, 0]])
        edge_weight = torch.FloatTensor([0.5, 0.6, 0.7, 0.8])

        output = backbone(x, edge_index, edge_weight)

        print("‚úÖ Backbone component test passed!")
        print(f"   - Output keys: {list(output.keys())}")
        print(
            f"   - Layer embeddings: {list(output['layer_embeddings'])}"
        )

        return True

    except Exception as e:
        print(f"‚ùå Backbone test failed: {e}")
        return False


if __name__ == "__main__":
    print("üöÄ Starting Simplified GIMAN Phase 1 tests...")

    # Run main tests
    success = test_simplified_giman()

    if success:
        # Test components
        test_model_components()

        print("\nüéØ Phase 1 Core Implementation Complete!")
        print("Next steps:")
        print("1. Integrate with real PPMI preprocessed data")
        print("2. Implement training loop and loss functions")
        print("3. Add validation metrics and evaluation")
        print("4. Create experiment tracking")
    else:
        print("\n‚ùå Phase 1 tests failed. Please fix errors.")
        sys.exit(1)
