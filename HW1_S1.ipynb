{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b506ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | x‚ÇÅ | x‚ÇÇ | t | x = (1, x‚ÇÅ, x‚ÇÇ) | Current w = (w‚ÇÄ, w‚ÇÅ, w‚ÇÇ) | w ¬∑ x | y | e | Œîw = e ¬∑ x      | New w = (w‚ÇÄ, w‚ÇÅ, w‚ÇÇ) |\n",
      "|:-----:|:--:|:--:|:-:|:---------------:|:-------------------------:|:-----:|:-:|:-:|:----------------:|:---------------------:|\n",
      "|       |    |    |   |                 | **(0, 0, 0)** |       |   |   |                  |                       |\n",
      "| **1** | 0  | 0  | 1 | (1, 0, 0)       | (0, 0, 0)                 |     0 | 1 | 0 | (0, 0, 0)        | (0, 0, 0)             |\n",
      "| **1** | 0  | 1  | 0 | (1, 0, 1)       | (0, 0, 0)                 |     0 | 1 | -1 | (-1, 0, -1)      | (-1, 0, -1)           |\n",
      "| **1** | 1  | 0  | 0 | (1, 1, 0)       | (-1, 0, -1)               |    -1 | 0 | 0 | (0, 0, 0)        | (-1, 0, -1)           |\n",
      "| **1** | 1  | 1  | 0 | (1, 1, 1)       | (-1, 0, -1)               |    -2 | 0 | 0 | (0, 0, 0)        | (-1, 0, -1)           |\n",
      "| **2** | 0  | 0  | 1 | (1, 0, 0)       | (-1, 0, -1)               |    -1 | 0 | 1 | (1, 0, 0)        | (0, 0, -1)            |\n",
      "| **2** | 0  | 1  | 0 | (1, 0, 1)       | (0, 0, -1)                |    -1 | 0 | 0 | (0, 0, 0)        | (0, 0, -1)            |\n",
      "| **2** | 1  | 0  | 0 | (1, 1, 0)       | (0, 0, -1)                |     0 | 1 | -1 | (-1, -1, 0)      | (-1, -1, -1)          |\n",
      "| **2** | 1  | 1  | 0 | (1, 1, 1)       | (-1, -1, -1)              |    -3 | 0 | 0 | (0, 0, 0)        | (-1, -1, -1)          |\n",
      "| **3** | 0  | 0  | 1 | (1, 0, 0)       | (-1, -1, -1)              |    -1 | 0 | 1 | (1, 0, 0)        | (0, -1, -1)           |\n",
      "| **3** | 0  | 1  | 0 | (1, 0, 1)       | (0, -1, -1)               |    -1 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **3** | 1  | 0  | 0 | (1, 1, 0)       | (0, -1, -1)               |    -1 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **3** | 1  | 1  | 0 | (1, 1, 1)       | (0, -1, -1)               |    -2 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **4** | 0  | 0  | 1 | (1, 0, 0)       | (0, -1, -1)               |     0 | 1 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **4** | 0  | 1  | 0 | (1, 0, 1)       | (0, -1, -1)               |    -1 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **4** | 1  | 0  | 0 | (1, 1, 0)       | (0, -1, -1)               |    -1 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "| **4** | 1  | 1  | 0 | (1, 1, 1)       | (0, -1, -1)               |    -2 | 0 | 0 | (0, 0, 0)        | (0, -1, -1)           |\n",
      "\n",
      "Convergence reached in Epoch 4. No further updates.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Setup ---\n",
    "# Inputs (x1, x2)\n",
    "inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "# Target outputs for x1 NOR x2\n",
    "targets = np.array([1, 0, 0, 0])\n",
    "\n",
    "# Add bias input (x0 = 1)\n",
    "X = np.insert(inputs, 0, 1, axis=1)\n",
    "\n",
    "# --- Training Parameters ---\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "learning_rate = 1\n",
    "max_epochs = 10 # Set a max number of epochs to prevent infinite loops\n",
    "\n",
    "# Store training data for CSV export\n",
    "training_data = []\n",
    "\n",
    "# --- Table Header ---\n",
    "header = \"| Epoch | x‚ÇÅ | x‚ÇÇ | t | x = (1, x‚ÇÅ, x‚ÇÇ) | Current w = (w‚ÇÄ, w‚ÇÅ, w‚ÇÇ) | w ¬∑ x | y | e | Œîw = e ¬∑ x      | New w = (w‚ÇÄ, w‚ÇÅ, w‚ÇÇ) |\"\n",
    "separator = \"|:-----:|:--:|:--:|:-:|:---------------:|:-------------------------:|:-----:|:-:|:-:|:----------------:|:---------------------:|\"\n",
    "print(header)\n",
    "print(separator)\n",
    "\n",
    "# Initial state print\n",
    "initial_w_str = f\"({weights[0]:.0f}, {weights[1]:.0f}, {weights[2]:.0f})\"\n",
    "print(f\"|       |    |    |   |                 | **{initial_w_str}** |       |   |   |                  |                       |\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    updates_in_epoch = 0\n",
    "    for i in range(len(X)):\n",
    "        x_vec = X[i]\n",
    "        target = targets[i]\n",
    "        \n",
    "        # Store current weights for printing\n",
    "        current_w_str = f\"({weights[0]:.0f}, {weights[1]:.0f}, {weights[2]:.0f})\"\n",
    "        \n",
    "        # 1. Calculate net input\n",
    "        net_input = np.dot(weights, x_vec)\n",
    "        \n",
    "        # 2. Apply step function\n",
    "        y = 1 if net_input >= 0 else 0\n",
    "        \n",
    "        # 3. Calculate error\n",
    "        error = target - y\n",
    "        \n",
    "        # 4. Calculate weight update\n",
    "        delta_w = learning_rate * error * x_vec\n",
    "        \n",
    "        # Store data for CSV\n",
    "        training_data.append({\n",
    "            'Epoch': epoch,\n",
    "            'x1': x_vec[1],\n",
    "            'x2': x_vec[2],\n",
    "            'target': target,\n",
    "            'x0': x_vec[0],\n",
    "            'w0_current': float(current_w_str.split(',')[0].strip('(').strip()),\n",
    "            'w1_current': float(current_w_str.split(',')[1].strip()),\n",
    "            'w2_current': float(current_w_str.split(',')[2].strip(')').strip()),\n",
    "            'net_input': net_input,\n",
    "            'y': y,\n",
    "            'error': error,\n",
    "            'delta_w0': delta_w[0],\n",
    "            'delta_w1': delta_w[1],\n",
    "            'delta_w2': delta_w[2],\n",
    "            'w0_new': weights[0] + delta_w[0],\n",
    "            'w1_new': weights[1] + delta_w[1],\n",
    "            'w2_new': weights[2] + delta_w[2]\n",
    "        })\n",
    "        \n",
    "        # 5. Update weights\n",
    "        if error != 0:\n",
    "            updates_in_epoch += 1\n",
    "        weights += delta_w\n",
    "        \n",
    "        # --- Print table row ---\n",
    "        x_str = f\"({x_vec[0]}, {x_vec[1]}, {x_vec[2]})\"\n",
    "        delta_w_str = f\"({delta_w[0]:.0f}, {delta_w[1]:.0f}, {delta_w[2]:.0f})\"\n",
    "        new_w_str = f\"({weights[0]:.0f}, {weights[1]:.0f}, {weights[2]:.0f})\"\n",
    "\n",
    "        print(f\"| **{epoch}** | {x_vec[1]}  | {x_vec[2]}  | {target} | {x_str: <15} | {current_w_str: <25} | {net_input: >5.0f} | {y} | {error: >1.0f} | {delta_w_str: <16} | {new_w_str: <21} |\")\n",
    "\n",
    "    # Check for convergence\n",
    "    if updates_in_epoch == 0:\n",
    "        print(f\"\\nConvergence reached in Epoch {epoch}. No further updates.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9658592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to 'perceptron_training_data.csv'\n",
      "Data shape: (16, 17)\n",
      "\n",
      "First few rows:\n",
      "   Epoch  x1  x2  target  x0  w0_current  w1_current  w2_current  net_input  \\\n",
      "0      1   0   0       1   1         0.0         0.0         0.0        0.0   \n",
      "1      1   0   1       0   1         0.0         0.0         0.0        0.0   \n",
      "2      1   1   0       0   1        -1.0         0.0        -1.0       -1.0   \n",
      "3      1   1   1       0   1        -1.0         0.0        -1.0       -2.0   \n",
      "4      2   0   0       1   1        -1.0         0.0        -1.0       -1.0   \n",
      "\n",
      "   y  error  delta_w0  delta_w1  delta_w2  w0_new  w1_new  w2_new  \n",
      "0  1      0         0         0         0     0.0     0.0     0.0  \n",
      "1  1     -1        -1         0        -1    -1.0     0.0    -1.0  \n",
      "2  0      0         0         0         0    -1.0     0.0    -1.0  \n",
      "3  0      0         0         0         0    -1.0     0.0    -1.0  \n",
      "4  0      1         1         0         0     0.0     0.0    -1.0  \n"
     ]
    }
   ],
   "source": [
    "# Save training data to CSV file\n",
    "df = pd.DataFrame(training_data)\n",
    "df.to_csv('perceptron_training_data.csv', index=False)\n",
    "print(\"Training data saved to 'perceptron_training_data.csv'\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690296ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /opt/anaconda3/lib/python3.12/site-packages (0.20.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b67d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: TLU_Network Pages: 1 -->\n",
       "<svg width=\"808pt\" height=\"530pt\"\n",
       " viewBox=\"0.00 0.00 807.50 529.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 525.5)\">\n",
       "<title>TLU_Network</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-525.5 803.5,-525.5 803.5,4 -4,4\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.75\" y=\"-7.5\" font-family=\"Times,serif\" font-size=\"20.00\">TLU Network for Triangle Classification</text>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_1</title>\n",
       "<polygon fill=\"none\" stroke=\"lightgrey\" points=\"360.75,-38.5 360.75,-513.5 507.56,-513.5 507.56,-38.5 360.75,-38.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-490.5\" font-family=\"Times,serif\" font-size=\"20.00\">Hidden Layer</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"lightgrey\" points=\"527.56,-168.5 527.56,-382.5 791.5,-382.5 791.5,-168.5 527.56,-168.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"659.53\" y=\"-359.5\" font-family=\"Times,serif\" font-size=\"20.00\">Output Layer</text>\n",
       "</g>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"none\" points=\"196.88,-332.5 142.88,-332.5 142.88,-296.5 196.88,-296.5 196.88,-332.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.88\" y=\"-308.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x‚ÇÅ</text>\n",
       "</g>\n",
       "<!-- TLU1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>TLU1</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"434.16\" cy=\"-260.5\" rx=\"65.41\" ry=\"65.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-262.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">TLU‚ÇÅ</text>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-247.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">w = (3, 0, &#45;1)</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;TLU1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x1&#45;&gt;TLU1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.5,-309.22C233.7,-301.56 304.16,-287.06 358.57,-275.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.99,-279.34 368.08,-273.9 357.58,-272.49 358.99,-279.34\"/>\n",
       "</g>\n",
       "<!-- TLU2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>TLU2</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"434.16\" cy=\"-409.5\" rx=\"65.41\" ry=\"65.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-411.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">TLU‚ÇÇ</text>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-396.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">w = (&#45;5, 2, 1)</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;TLU2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1&#45;&gt;TLU2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.5,-323.78C234.39,-337.5 306.76,-363.72 361.54,-383.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"360.23,-386.81 370.82,-386.92 362.61,-380.23 360.23,-386.81\"/>\n",
       "</g>\n",
       "<!-- TLU3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>TLU3</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"lightblue\" cx=\"434.16\" cy=\"-111.5\" rx=\"65.41\" ry=\"65.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-113.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">TLU‚ÇÉ</text>\n",
       "<text text-anchor=\"middle\" x=\"434.16\" y=\"-98.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">w = (3, &#45;2, 1)</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;TLU3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x1&#45;&gt;TLU3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.69,-296.84C233.26,-266.21 315.85,-202.29 372.88,-158.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.84,-161.06 380.61,-152.17 370.56,-155.53 374.84,-161.06\"/>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"none\" points=\"196.88,-224.5 142.88,-224.5 142.88,-188.5 196.88,-188.5 196.88,-224.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.88\" y=\"-200.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x‚ÇÇ</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;TLU1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x2&#45;&gt;TLU1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.5,-211.78C233.7,-219.44 304.16,-233.94 358.57,-245.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"357.58,-248.51 368.08,-247.1 358.99,-241.66 357.58,-248.51\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;TLU2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>x2&#45;&gt;TLU2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M193.69,-224.16C233.26,-254.79 315.85,-318.71 372.88,-362.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"370.56,-365.47 380.61,-368.83 374.84,-359.94 370.56,-365.47\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;TLU3 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x2&#45;&gt;TLU3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.5,-197.22C234.39,-183.5 306.76,-157.28 361.54,-137.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.61,-140.77 370.82,-134.08 360.23,-134.19 362.61,-140.77\"/>\n",
       "</g>\n",
       "<!-- bias_in -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>bias_in</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"none\" points=\"205.25,-278.5 134.5,-278.5 134.5,-242.5 205.25,-242.5 205.25,-278.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.88\" y=\"-254.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Bias (+1)</text>\n",
       "</g>\n",
       "<!-- bias_in&#45;&gt;TLU1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>bias_in&#45;&gt;TLU1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M205.16,-260.5C243.49,-260.5 306.75,-260.5 356.92,-260.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"356.88,-264 366.88,-260.5 356.88,-257 356.88,-264\"/>\n",
       "</g>\n",
       "<!-- bias_in&#45;&gt;TLU2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>bias_in&#45;&gt;TLU2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.51,-278.47C243.02,-301.49 314.27,-341.96 366.89,-371.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.08,-374.85 375.5,-376.75 368.53,-368.76 365.08,-374.85\"/>\n",
       "</g>\n",
       "<!-- bias_in&#45;&gt;TLU3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>bias_in&#45;&gt;TLU3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.51,-242.53C243.02,-219.51 314.27,-179.04 366.89,-149.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.53,-152.24 375.5,-144.25 365.08,-146.15 368.53,-152.24\"/>\n",
       "</g>\n",
       "<!-- TLU_out -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>TLU_out</title>\n",
       "<ellipse fill=\"palegreen\" stroke=\"palegreen\" cx=\"619.53\" cy=\"-260.5\" rx=\"83.97\" ry=\"83.97\"/>\n",
       "<text text-anchor=\"middle\" x=\"619.53\" y=\"-262.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">TLU_out</text>\n",
       "<text text-anchor=\"middle\" x=\"619.53\" y=\"-247.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">w = (&#45;2.5, 1, 1, 1)</text>\n",
       "</g>\n",
       "<!-- TLU1&#45;&gt;TLU_out -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>TLU1&#45;&gt;TLU_out</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.81,-260.5C507.59,-260.5 515.65,-260.5 523.75,-260.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.68,-264 533.68,-260.5 523.68,-257 523.68,-264\"/>\n",
       "</g>\n",
       "<!-- TLU2&#45;&gt;TLU_out -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>TLU2&#45;&gt;TLU_out</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.27,-368.79C503.65,-353.85 524.86,-336.61 544.88,-320.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"546.94,-323.18 552.5,-314.16 542.53,-317.75 546.94,-323.18\"/>\n",
       "</g>\n",
       "<!-- TLU3&#45;&gt;TLU_out -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>TLU3&#45;&gt;TLU_out</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.27,-152.21C503.65,-167.15 524.86,-184.39 544.88,-200.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"542.53,-203.25 552.5,-206.84 546.94,-197.82 542.53,-203.25\"/>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"palegreen\" stroke=\"palegreen\" cx=\"761.5\" cy=\"-260.5\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"palegreen\" cx=\"761.5\" cy=\"-260.5\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"761.5\" y=\"-254.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- TLU_out&#45;&gt;Y -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>TLU_out&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M703.98,-260.5C712.3,-260.5 720.36,-260.5 727.67,-260.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"727.64,-264 737.64,-260.5 727.64,-257 727.64,-264\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x116f8fce0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "# Create a new directed graph\n",
    "g = graphviz.Digraph('TLU_Network')\n",
    "g.attr(rankdir='LR', splines='line', label='TLU Network for Triangle Classification', fontsize='20')\n",
    "\n",
    "# Define graph-wide styles for nodes and edges\n",
    "g.attr('node', shape='circle', style='filled', color='lightblue', fontname='Helvetica')\n",
    "g.attr('edge', fontname='Helvetica', fontsize='10')\n",
    "\n",
    "# 1. Input Layer\n",
    "# Use a subgraph to align the input nodes\n",
    "with g.subgraph(name='cluster_0') as c:\n",
    "    c.attr(style='invis') # Make the subgraph box invisible\n",
    "    c.node('x1', 'x‚ÇÅ', shape='plaintext')\n",
    "    c.node('x2', 'x‚ÇÇ', shape='plaintext')\n",
    "    c.node('bias_in', 'Bias (+1)', shape='plaintext')\n",
    "\n",
    "# 2. Hidden Layer (Boundary Detectors)\n",
    "with g.subgraph(name='cluster_1') as c:\n",
    "    c.attr(label='Hidden Layer', color='lightgrey')\n",
    "    c.node('TLU1', 'TLU‚ÇÅ\\nw = (3, 0, -1)')\n",
    "    c.node('TLU2', 'TLU‚ÇÇ\\nw = (-5, 2, 1)')\n",
    "    c.node('TLU3', 'TLU‚ÇÉ\\nw = (3, -2, 1)')\n",
    "\n",
    "# 3. Output Layer (AND Gate)\n",
    "with g.subgraph(name='cluster_2') as c:\n",
    "    c.attr(label='Output Layer', color='lightgrey')\n",
    "    c.node('TLU_out', 'TLU_out\\nw = (-2.5, 1, 1, 1)', color='palegreen')\n",
    "    c.node('Y', 'Y', shape='doublecircle', color='palegreen')\n",
    "\n",
    "\n",
    "# --- Define Edges (Connections) ---\n",
    "\n",
    "# Connections from Inputs to Hidden Layer\n",
    "g.edge('x1', 'TLU1')\n",
    "g.edge('x1', 'TLU2')\n",
    "g.edge('x1', 'TLU3')\n",
    "\n",
    "g.edge('x2', 'TLU1')\n",
    "g.edge('x2', 'TLU2')\n",
    "g.edge('x2', 'TLU3')\n",
    "\n",
    "g.edge('bias_in', 'TLU1')\n",
    "g.edge('bias_in', 'TLU2')\n",
    "g.edge('bias_in', 'TLU3')\n",
    "\n",
    "# Connections from Hidden Layer to Output Layer\n",
    "g.edge('TLU1', 'TLU_out')\n",
    "g.edge('TLU2', 'TLU_out')\n",
    "g.edge('TLU3', 'TLU_out')\n",
    "\n",
    "# Connection from Output TLU to the final result\n",
    "g.edge('TLU_out', 'Y')\n",
    "\n",
    "# Simply calling the object 'g' at the end of the cell\n",
    "# will render the diagram in the notebook's output.\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5c265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION IMPUTATION MODULE TEST\n",
    "# Test the production biomarker imputation pipeline from the codebase\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Testing Production Biomarker Imputation Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the production imputation pipeline from our codebase\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/src')\n",
    "\n",
    "from giman_pipeline.data_processing import BiommarkerImputationPipeline\n",
    "\n",
    "# Test the production pipeline on our dataset\n",
    "print(\"\\n1. Loading the dataset for production testing...\")\n",
    "# Use the same enhanced dataset from our previous work\n",
    "df_test = df_enhanced.copy()\n",
    "print(f\"Dataset shape: {df_test.shape}\")\n",
    "\n",
    "# Initialize the production imputation pipeline\n",
    "print(\"\\n2. Initializing production imputation pipeline...\")\n",
    "biomarker_imputer = BiommarkerImputationPipeline(\n",
    "    knn_neighbors=5,\n",
    "    mice_max_iter=10,\n",
    "    mice_random_state=42\n",
    ")\n",
    "\n",
    "# Analyze missingness using production pipeline\n",
    "print(\"\\n3. Analyzing missingness patterns...\")\n",
    "missingness_prod = biomarker_imputer.analyze_missingness(df_test)\n",
    "\n",
    "print(\"\\nMissingness Analysis (Production Pipeline):\")\n",
    "for biomarker, pct in missingness_prod.items():\n",
    "    print(f\"  {biomarker}: {pct:.1f}% missing\")\n",
    "\n",
    "# Categorize biomarkers by missingness level\n",
    "low_missing, moderate_missing, high_missing = biomarker_imputer.categorize_by_missingness(missingness_prod)\n",
    "\n",
    "print(f\"\\nMissingness Categories:\")\n",
    "print(f\"  Low missingness (<20%): {low_missing}\")\n",
    "print(f\"  Moderate missingness (40-55%): {moderate_missing}\")\n",
    "print(f\"  High missingness (>70%): {high_missing}\")\n",
    "\n",
    "# Fit and transform using production pipeline\n",
    "print(\"\\n4. Fitting and transforming with production pipeline...\")\n",
    "df_production_imputed = biomarker_imputer.fit_transform(df_test)\n",
    "\n",
    "# Get completion statistics\n",
    "print(\"\\n5. Calculating completion statistics...\")\n",
    "completion_stats = biomarker_imputer.get_completion_stats(df_test, df_production_imputed)\n",
    "\n",
    "print(f\"\\nProduction Pipeline Completion Results:\")\n",
    "print(f\"  Total patients: {completion_stats['total_patients']:,}\")\n",
    "print(f\"  Biomarkers analyzed: {completion_stats['biomarkers_analyzed']}\")\n",
    "print(f\"  Original complete profiles: {completion_stats['original_complete_profiles']:,} ({completion_stats['original_completion_rate']:.1%})\")\n",
    "print(f\"  Production imputed profiles: {completion_stats['imputed_complete_profiles']:,} ({completion_stats['imputed_completion_rate']:.1%})\")\n",
    "print(f\"  Improvement: {completion_stats['improvement']:.1%}\")\n",
    "\n",
    "# Get imputation summary\n",
    "print(\"\\n6. Getting imputation summary...\")\n",
    "imputation_summary = biomarker_imputer.get_imputation_summary()\n",
    "\n",
    "print(f\"\\nProduction Imputation Summary:\")\n",
    "print(f\"  Biomarkers processed: {imputation_summary['biomarkers_processed']}\")\n",
    "print(f\"  KNN imputation: {imputation_summary['imputation_strategies']['knn_imputation']}\")\n",
    "print(f\"  MICE imputation: {imputation_summary['imputation_strategies']['mice_imputation']}\")\n",
    "print(f\"  Cohort median imputation: {imputation_summary['imputation_strategies']['cohort_median_imputation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PRODUCTION IMPUTATION PIPELINE TEST COMPLETE\")\n",
    "print(\"‚úÖ Production module successfully achieves biomarker imputation!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6254fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of production imputation module import\n",
    "print(\"Testing production module import...\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/src')\n",
    "\n",
    "try:\n",
    "    from giman_pipeline.data_processing import BiommarkerImputationPipeline\n",
    "    print(\"‚úÖ Successfully imported BiommarkerImputationPipeline from production codebase!\")\n",
    "    \n",
    "    # Create instance to test basic functionality\n",
    "    imputer = BiommarkerImputationPipeline()\n",
    "    print(f\"‚úÖ Successfully created imputation pipeline instance\")\n",
    "    print(f\"   Default biomarkers: {imputer.biomarker_columns}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating imputation pipeline: {e}\")\n",
    "\n",
    "print(\"Production module import test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a087993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION SIMILARITY GRAPH CONSTRUCTION\n",
    "# Test the production PatientSimilarityGraph module from the codebase\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Testing Production Patient Similarity Graph Constructor\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the production similarity graph constructor\n",
    "try:\n",
    "    from giman_pipeline.modeling.patient_similarity import PatientSimilarityGraph, create_patient_similarity_graph\n",
    "    print(\"‚úÖ Successfully imported PatientSimilarityGraph from production codebase!\")\n",
    "    \n",
    "    # Test basic constructor initialization\n",
    "    similarity_constructor = PatientSimilarityGraph(\n",
    "        similarity_threshold=0.3,\n",
    "        similarity_metric=\"cosine\",\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"‚úÖ Successfully created similarity graph constructor\")\n",
    "    print(f\"   Biomarker features: {similarity_constructor.biomarker_features}\")\n",
    "    print(f\"   Similarity metric: {similarity_constructor.similarity_metric}\")\n",
    "    print(f\"   Similarity threshold: {similarity_constructor.similarity_threshold}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating similarity graph constructor: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD PRODUCTION SIMILARITY GRAPH FROM ENHANCED COHORT\n",
    "# Use the production pipeline to build the patient similarity graph\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Building Patient Similarity Graph with Production Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build complete similarity graph using production code\n",
    "print(\"\\nüî® Building similarity graph from 557-patient enhanced cohort...\")\n",
    "print(\"Using production PatientSimilarityGraph constructor...\")\n",
    "\n",
    "# Use the convenience function to build everything\n",
    "try:\n",
    "    # Parameters for similarity graph construction\n",
    "    similarity_threshold = 0.3  # Lower threshold for denser connections\n",
    "    similarity_metric = \"cosine\"  # Cosine similarity for biomarker features\n",
    "    save_results = True  # Save graph to 03_similarity_graphs directory\n",
    "    \n",
    "    # Build complete graph pipeline\n",
    "    production_graph, production_adjacency, production_metadata = create_patient_similarity_graph(\n",
    "        similarity_threshold=similarity_threshold,\n",
    "        similarity_metric=similarity_metric,\n",
    "        save_results=save_results,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SIMILARITY GRAPH CONSTRUCTION COMPLETE!\")\n",
    "    print(f\"\\nüìä Graph Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Patients (nodes): {production_metadata['graph_nodes']}\")\n",
    "    print(f\"   ‚Ä¢ Connections (edges): {production_metadata['graph_edges']}\")\n",
    "    print(f\"   ‚Ä¢ Graph density: {production_metadata['graph_density']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Average degree: {production_metadata['avg_degree']:.1f}\")\n",
    "    print(f\"   ‚Ä¢ Max degree: {production_metadata['max_degree']}\")\n",
    "    print(f\"   ‚Ä¢ Connected: {production_metadata['is_connected']}\")\n",
    "    \n",
    "    print(f\"\\nüî¨ Biomarker Features Used:\")\n",
    "    for i, feature in enumerate(production_metadata['biomarker_features'], 1):\n",
    "        print(f\"   {i}. {feature}\")\n",
    "        \n",
    "    print(f\"\\nüìà Similarity Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Mean similarity: {production_metadata['similarity_mean']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Std similarity: {production_metadata['similarity_std']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Min similarity: {production_metadata['similarity_min']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Max similarity: {production_metadata['similarity_max']:.3f}\")\n",
    "    \n",
    "    if 'n_communities' in production_metadata:\n",
    "        print(f\"\\nüèòÔ∏è Community Detection:\")\n",
    "        print(f\"   ‚Ä¢ Communities detected: {production_metadata['n_communities']}\")\n",
    "        print(f\"   ‚Ä¢ Modularity score: {production_metadata['modularity']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Data Quality:\")\n",
    "    print(f\"   ‚Ä¢ Patient count: {production_metadata['patient_count']}\")\n",
    "    print(f\"   ‚Ä¢ Data completeness: {production_metadata['data_completeness_percent']:.1f}%\")\n",
    "    \n",
    "    if 'saved_to' in production_metadata:\n",
    "        print(f\"\\nüìÅ Results saved to: {production_metadata['saved_to']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ PRODUCTION SIMILARITY GRAPH PIPELINE COMPLETE!\")\n",
    "    print(\"‚úÖ Production module successfully constructs patient similarity graph!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in similarity graph construction: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull error traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION OF PRODUCTION SIMILARITY GRAPH\n",
    "# Create comprehensive visualizations to validate the production graph\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating Production Similarity Graph Visualizations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "\n",
    "# Use the production graph for visualization\n",
    "G = production_graph\n",
    "similarity_matrix = None  # We'll extract from graph if needed\n",
    "\n",
    "print(f\"\\nüìä Visualizing graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Create comprehensive multi-panel visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Production Patient Similarity Graph Analysis (557 Patients)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Panel 1: Main similarity graph with spring layout\n",
    "print(\"üé® Creating main graph visualization...\")\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "# Use spring layout for main visualization\n",
    "pos = nx.spring_layout(G, k=1.5, iterations=50, seed=42)\n",
    "\n",
    "# Extract node colors based on cohort (if available)\n",
    "node_colors = []\n",
    "cohort_labels = {}\n",
    "for node in G.nodes():\n",
    "    cohort = G.nodes[node].get('cohort', 'Unknown')\n",
    "    if cohort == 'Parkinson\\'s Disease':\n",
    "        node_colors.append('#d62728')  # Red for PD\n",
    "        cohort_labels[node] = 'PD'\n",
    "    elif cohort == 'Healthy Control':\n",
    "        node_colors.append('#2ca02c')  # Green for HC\n",
    "        cohort_labels[node] = 'HC'\n",
    "    else:\n",
    "        node_colors.append('#1f77b4')  # Blue for unknown\n",
    "        cohort_labels[node] = 'Unknown'\n",
    "\n",
    "# Draw the network\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=30, alpha=0.7, ax=ax1)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.3, width=0.5, ax=ax1)\n",
    "\n",
    "ax1.set_title('Patient Similarity Network\\n(Node color = Cohort)', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Add legend for cohorts\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#d62728', label=\"Parkinson's Disease\"),\n",
    "    Patch(facecolor='#2ca02c', label='Healthy Control'),\n",
    "    Patch(facecolor='#1f77b4', label='Unknown/Other')\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "\n",
    "# Panel 2: Degree distribution\n",
    "print(\"üìà Computing degree distribution...\")\n",
    "ax2 = axes[0, 1]\n",
    "degrees = [d for n, d in G.degree()]\n",
    "degree_counts = Counter(degrees)\n",
    "degrees_sorted = sorted(degree_counts.keys())\n",
    "counts = [degree_counts[d] for d in degrees_sorted]\n",
    "\n",
    "ax2.bar(degrees_sorted, counts, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax2.set_title('Degree Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Degree (Number of Connections)')\n",
    "ax2.set_ylabel('Number of Patients')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics to degree plot\n",
    "mean_degree = np.mean(degrees)\n",
    "max_degree = max(degrees)\n",
    "ax2.axvline(mean_degree, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_degree:.1f}')\n",
    "ax2.axvline(max_degree, color='orange', linestyle='--', linewidth=2, label=f'Max: {max_degree}')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Panel 3: Similarity distribution (extract from edge weights)\n",
    "print(\"üìä Analyzing edge weight distribution...\")\n",
    "ax3 = axes[0, 2]\n",
    "edge_similarities = [data['similarity'] for u, v, data in G.edges(data=True) if 'similarity' in data]\n",
    "\n",
    "if edge_similarities:\n",
    "    ax3.hist(edge_similarities, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax3.set_title('Edge Similarity Distribution', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Similarity Score')\n",
    "    ax3.set_ylabel('Number of Edges')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_sim = np.mean(edge_similarities)\n",
    "    std_sim = np.std(edge_similarities)\n",
    "    ax3.axvline(mean_sim, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_sim:.3f}')\n",
    "    ax3.axvline(mean_sim + std_sim, color='orange', linestyle='--', linewidth=1, label=f'+1œÉ: {mean_sim + std_sim:.3f}')\n",
    "    ax3.legend(fontsize=8)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No similarity data\\navailable in edges', \n",
    "             ha='center', va='center', transform=ax3.transAxes, fontsize=12)\n",
    "    ax3.set_title('Edge Similarity Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Panel 4: Community detection visualization (if communities detected)\n",
    "ax4 = axes[1, 0]\n",
    "if 'communities' in production_metadata and production_metadata['communities']:\n",
    "    print(\"üèòÔ∏è Visualizing community structure...\")\n",
    "    communities = production_metadata['communities']\n",
    "    \n",
    "    # Create community colors\n",
    "    n_communities = len(set(communities.values()))\n",
    "    community_colors = plt.cm.Set3(np.linspace(0, 1, n_communities))\n",
    "    \n",
    "    # Color nodes by community\n",
    "    node_colors_community = []\n",
    "    for node in G.nodes():\n",
    "        community_id = communities.get(node, -1)\n",
    "        if community_id >= 0:\n",
    "            node_colors_community.append(community_colors[community_id])\n",
    "        else:\n",
    "            node_colors_community.append('gray')\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors_community, node_size=30, alpha=0.8, ax=ax4)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.2, width=0.5, ax=ax4)\n",
    "    \n",
    "    ax4.set_title(f'Community Structure\\n({n_communities} communities, modularity: {production_metadata.get(\"modularity\", 0):.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax4.axis('off')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Community detection\\nnot available', \n",
    "             ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.set_title('Community Structure', fontsize=12, fontweight='bold')\n",
    "    ax4.axis('off')\n",
    "\n",
    "# Panel 5: Cohort homogeneity analysis\n",
    "print(\"üî¨ Analyzing cohort homogeneity...\")\n",
    "ax5 = axes[1, 1]\n",
    "\n",
    "# Count cohort distribution\n",
    "cohort_distribution = {}\n",
    "for node in G.nodes():\n",
    "    cohort = G.nodes[node].get('cohort', 'Unknown')\n",
    "    cohort_distribution[cohort] = cohort_distribution.get(cohort, 0) + 1\n",
    "\n",
    "# Create pie chart of cohort distribution\n",
    "cohorts = list(cohort_distribution.keys())\n",
    "counts = list(cohort_distribution.values())\n",
    "colors = ['#d62728' if 'Parkinson' in cohort else '#2ca02c' if 'Healthy' in cohort else '#1f77b4' \n",
    "          for cohort in cohorts]\n",
    "\n",
    "wedges, texts, autotexts = ax5.pie(counts, labels=cohorts, colors=colors, autopct='%1.1f%%', \n",
    "                                   startangle=90, textprops={'fontsize': 8})\n",
    "ax5.set_title('Cohort Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Panel 6: Network properties summary\n",
    "ax6 = axes[1, 2]\n",
    "ax6.axis('off')\n",
    "\n",
    "# Calculate additional network properties\n",
    "try:\n",
    "    clustering = nx.average_clustering(G)\n",
    "    if nx.is_connected(G):\n",
    "        avg_path_length = nx.average_shortest_path_length(G)\n",
    "        diameter = nx.diameter(G)\n",
    "    else:\n",
    "        # For disconnected graphs, calculate for largest component\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G_largest = G.subgraph(largest_cc)\n",
    "        avg_path_length = nx.average_shortest_path_length(G_largest)\n",
    "        diameter = nx.diameter(G_largest)\n",
    "    \n",
    "    n_components = nx.number_connected_components(G)\n",
    "    \n",
    "    properties_text = f'''Network Properties:\n",
    "    \n",
    "‚Ä¢ Nodes: {G.number_of_nodes()}\n",
    "‚Ä¢ Edges: {G.number_of_edges()}\n",
    "‚Ä¢ Density: {nx.density(G):.4f}\n",
    "‚Ä¢ Avg. Clustering: {clustering:.3f}\n",
    "‚Ä¢ Avg. Path Length: {avg_path_length:.2f}\n",
    "‚Ä¢ Diameter: {diameter}\n",
    "‚Ä¢ Components: {n_components}\n",
    "‚Ä¢ Connected: {nx.is_connected(G)}\n",
    "\n",
    "Biomarker Features:\n",
    "‚Ä¢ CSF Tau, pTau, AŒ≤42\n",
    "‚Ä¢ Serum NfL \n",
    "‚Ä¢ APOE Œµ4 status\n",
    "‚Ä¢ Age at visit\n",
    "‚Ä¢ Sex (numeric)\n",
    "\n",
    "Similarity Metric: {production_metadata.get('similarity_metric', 'cosine')}\n",
    "Threshold: {production_metadata.get('similarity_threshold', 0.3)}\n",
    "Data Completeness: {production_metadata.get('data_completeness_percent', 0):.1f}%'''\n",
    "    \n",
    "    ax6.text(0.05, 0.95, properties_text, transform=ax6.transAxes, fontsize=9,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "             \n",
    "except Exception as e:\n",
    "    ax6.text(0.5, 0.5, f'Error calculating\\nnetwork properties:\\n{str(e)}', \n",
    "             ha='center', va='center', transform=ax6.transAxes, fontsize=10)\n",
    "\n",
    "ax6.set_title('Graph Properties Summary', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ VISUALIZATION COMPLETE!\")\n",
    "print(f\"üìä Successfully visualized production similarity graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} connections\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify the production module is available\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src path\n",
    "src_path = '/Users/blair.dupre/Library/CloudStorage/GoogleDrive-dupre.blair92@gmail.com/My Drive/CSCI FALL 2025/src'\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "print(f\"Python path includes src: {src_path in sys.path}\")\n",
    "print(f\"File exists: {os.path.exists(src_path + '/giman_pipeline/modeling/patient_similarity.py')}\")\n",
    "\n",
    "try:\n",
    "    from giman_pipeline.modeling.patient_similarity import PatientSimilarityGraph\n",
    "    print(\"‚úÖ Import successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
